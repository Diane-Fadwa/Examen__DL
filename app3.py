import streamlit as st
import ollama

# Titre de l'application
st.title("Consultant RH Virtuel ü§ù")

# Section pour l'objectif utilisateur
st.sidebar.header("Objectif de la session")
goals = ["Pr√©parer un entretien", "Am√©liorer mon CV", "R√©ponses aux questions de carri√®re"]
selected_goal = st.sidebar.selectbox("Choisissez votre objectif", goals)

# Options de configuration
st.sidebar.header("Options de configuration")
available_models = ["llama3.2", "qwen:0.5b"]
selected_model = st.sidebar.selectbox("Mod√®le LLM", available_models, index=0)
temperature = st.sidebar.slider("Temp√©rature", 0.0, 2.0, 0.7, 0.1)
top_k = st.sidebar.slider("Top-k", 1, 100, 40, 1)
top_p = st.sidebar.slider("Top-p", 0.0, 1.0, 0.9, 0.05)
max_tokens = st.sidebar.slider("Nombre maximal de tokens", 1, 2048, 512, 1)

# V√©rifier la connexion au mod√®le LLM
try:
    test_response = ollama.chat(
        model=selected_model,
        messages=[{"role": "user", "content": "Test de connexion au mod√®le"}]
    )
    st.sidebar.success(f"Le mod√®le '{selected_model}' fonctionne correctement !")
except Exception as e:
    st.sidebar.error(f"Erreur de connexion au mod√®le : {e}")

# Initialiser l'historique des messages
if "messages" not in st.session_state:
    st.session_state.messages = []

# Fonction pour d√©tecter les √©motions dans les messages
def detect_emotion_from_message(message):
    """D√©tecte les √©motions bas√©es sur des mots-cl√©s en contexte RH."""
    if any(word in message.lower() for word in ["stress", "nerveux", "anxieux", "doute"]):
        return "stress"
    elif any(word in message.lower() for word in ["confiant", "positif", "pr√™t"]):
        return "confiance"
    elif any(word in message.lower() for word in ["perdu", "confus", "difficile"]):
        return "confusion"
    else:
        return "neutre"

# Fonction pour g√©n√©rer des r√©ponses personnalis√©es
def generate_personalized_response(emotion, response):
    """Ajoute une personnalisation en fonction de l'√©motion d√©tect√©e."""
    if emotion == "stress":
        return f"Je ressens une certaine nervosit√©. Pas de souci, voici un conseil utile : {response}"
    elif emotion == "confiance":
        return f"Vous semblez tr√®s confiant, bravo ! Continuez ainsi : {response}"
    elif emotion == "confusion":
        return f"Il semble que vous ayez besoin d'√©claircissements. Voici une explication d√©taill√©e : {response}"
    else:
        return response

# Afficher les messages existants dans la conversation
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entr√©e utilisateur
prompt = st.chat_input("Posez votre question ou simulez un entretien :")
if prompt:
    # Ajouter la question de l'utilisateur dans l'interface
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # D√©tecter l'√©motion associ√©e au message
    emotion = detect_emotion_from_message(prompt)

    # G√©n√©rer une r√©ponse avec le mod√®le
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        # Appeler le mod√®le LLM avec les options configur√©es
        for response in ollama.chat(
            model=selected_model,
            messages=st.session_state.messages,
            options={
                "temperature": temperature,
                "top_k": top_k,
                "top_p": top_p,
                "num_predict": max_tokens,
            },
            stream=True
        ):
            full_response += response["message"]["content"]
            message_placeholder.markdown(full_response + "‚ñå")  # Animation pendant la g√©n√©ration
        message_placeholder.markdown(full_response)

    # Ajouter une personnalisation bas√©e sur l'√©motion
    personalized_response = generate_personalized_response(emotion, full_response)

    # Afficher et enregistrer la r√©ponse dans l'historique
    with st.chat_message("assistant"):
        st.markdown(personalized_response)
    st.session_state.messages.append({"role": "assistant", "content": personalized_response})
